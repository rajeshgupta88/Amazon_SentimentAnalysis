{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import plotly.graph_objs as go\n",
    "#import plotly.offline as py\n",
    "#py.init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 3000000 #number of records in file\n",
    "s = 30000 #desired sample size\n",
    "filename = \"../Data/amazon_review_full_csv/train.csv\"\n",
    "skip = sorted(random.sample(range(n),n-s))\n",
    "\n",
    "data_df = pd.read_csv(filename, header=None,skiprows=skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't buy from Eforcity -- slow, disingenuous</td>\n",
       "      <td>What am I reviewing here... this gizmo or the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Nice Thomas Sampler</td>\n",
       "      <td>Marking the 10th Anniversary of Thomas videos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Wrong item sent.</td>\n",
       "      <td>I got the shippment quickly. But wrong items w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Riveting!</td>\n",
       "      <td>Don't let the first quarter of this book disco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Negative Stars! Avoid like flesh eating bacteria!</td>\n",
       "      <td>\"Lets jiggel the camera to show that the groun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1  \\\n",
       "0  1      Don't buy from Eforcity -- slow, disingenuous   \n",
       "1  4                                Nice Thomas Sampler   \n",
       "2  2                                   Wrong item sent.   \n",
       "3  3                                          Riveting!   \n",
       "4  1  Negative Stars! Avoid like flesh eating bacteria!   \n",
       "\n",
       "                                                   2  \n",
       "0  What am I reviewing here... this gizmo or the ...  \n",
       "1  Marking the 10th Anniversary of Thomas videos ...  \n",
       "2  I got the shippment quickly. But wrong items w...  \n",
       "3  Don't let the first quarter of this book disco...  \n",
       "4  \"Lets jiggel the camera to show that the groun...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df.columns =['Rating','Review Title', 'Review Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't buy from Eforcity -- slow, disingenuous</td>\n",
       "      <td>What am I reviewing here... this gizmo or the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Nice Thomas Sampler</td>\n",
       "      <td>Marking the 10th Anniversary of Thomas videos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Wrong item sent.</td>\n",
       "      <td>I got the shippment quickly. But wrong items w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Riveting!</td>\n",
       "      <td>Don't let the first quarter of this book disco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Negative Stars! Avoid like flesh eating bacteria!</td>\n",
       "      <td>\"Lets jiggel the camera to show that the groun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                       Review Title  \\\n",
       "0       1      Don't buy from Eforcity -- slow, disingenuous   \n",
       "1       4                                Nice Thomas Sampler   \n",
       "2       2                                   Wrong item sent.   \n",
       "3       3                                          Riveting!   \n",
       "4       1  Negative Stars! Avoid like flesh eating bacteria!   \n",
       "\n",
       "                                         Review Text  \n",
       "0  What am I reviewing here... this gizmo or the ...  \n",
       "1  Marking the 10th Anniversary of Thomas videos ...  \n",
       "2  I got the shippment quickly. But wrong items w...  \n",
       "3  Don't let the first quarter of this book disco...  \n",
       "4  \"Lets jiggel the camera to show that the groun...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Rating'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.51175   ,  0.        ,  0.50408333,  0.        ,  0.        ,\n",
       "         0.49066667,  0.        ,  0.50033333,  0.        ,  0.49316667]),\n",
       " array([ 1. ,  1.4,  1.8,  2.2,  2.6,  3. ,  3.4,  3.8,  4.2,  4.6,  5. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3CWpQcZdkhpk3CWpQcZdkhr0f5kPPBF9zN4AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1521e4f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data_df['Rating'], align='mid', normed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting rating into Binary class\n",
    "data_df['Rating']= data_df['Rating'].replace([1,2],0)\n",
    "data_df['Rating']= data_df['Rating'].replace([3,4,5],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 12190.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,  17810.]),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWm0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1520e7b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data_df['Rating'], align='mid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reviews_to_words (raw_reviews):\n",
    "    \n",
    "    # Remove the HTML characters\n",
    "    reviews_text = BeautifulSoup(raw_reviews)\n",
    "    \n",
    "    # Remove the punctuals\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",\" \",reviews_text.get_text())\n",
    "    \n",
    "    # Convert it in lower case and split them into words\n",
    "    lower_case = letters_only.lower()\n",
    "    words = lower_case.split()\n",
    "    \n",
    "    #In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    \n",
    "    meaningfull_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    \n",
    "    #Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningfull_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This disc was played everyweekend at our parties, so much so it was almost a ritual. Not quite as good as Love Machine but still an awesome disc.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Review Text'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajesh/anaconda2/envs/py36/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning:\n",
      "\n",
      "No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/rajesh/anaconda2/envs/py36/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'disc played everyweekend parties much almost ritual quite good love machine still awesome disc'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check one example\n",
    "reviews_to_words(data_df['Review Text'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajesh/anaconda2/envs/py36/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning:\n",
      "\n",
      "No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/rajesh/anaconda2/envs/py36/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 30000\n",
      "\n",
      "Review 10000 of 30000\n",
      "\n",
      "Review 15000 of 30000\n",
      "\n",
      "Review 20000 of 30000\n",
      "\n",
      "Review 25000 of 30000\n",
      "\n",
      "Review 30000 of 30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_reviews= data_df['Review Text'].size\n",
    "\n",
    "clean_reviews_all= []\n",
    "\n",
    "for i in range(0, num_reviews):\n",
    "    # If the index is evenly divisible by 400, print a message\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % ( i+1, num_reviews))\n",
    "    \n",
    "    clean_reviews_all.append(reviews_to_words(data_df['Review Text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read book th grade lit class must admit book quite shock many books read nothing attempt delving inner parts human soul book much depicts struggle man outside enemy commits one unspeakable acts time punishes constantly lacks courage confess crime goes period mental physical torture also faces wrath cruel vicious enemy lives nothing torture victim believe excellent book would reccomend anyone'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews_all[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "clean_reviews_all_1= [[porter_stemmer.stem(word) for word in sentence.split(\" \")] for sentence in clean_reviews_all]\n",
    "clean_reviews_all_1= [\" \".join(sentence) for sentence in clean_reviews_all_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read book th grade lit class must admit book quit shock mani book read noth attempt delv inner part human soul book much depict struggl man outsid enemi commit one unspeak act time punish constantli lack courag confess crime goe period mental physic tortur also face wrath cruel viciou enemi live noth tortur victim believ excel book would reccomend anyon'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews_all_1[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a corpus of all words\n",
    "\n",
    "all_text = ''.join([c for c in clean_reviews_all_1])\n",
    "reviews = all_text.split(',')\n",
    "\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for associating a integer value to each unique word\n",
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key = counts.get, reverse = True)\n",
    "vocab_to_int = {word:ii for ii , word in enumerate(vocab,1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map the integer values to the words in each review using above dictionary\n",
    "reviews_ints =[]\n",
    "for r in clean_reviews_all_1:\n",
    "    ri = [vocab_to_int.get(w) for w in r.split(\" \") if vocab_to_int.get(w) is not None]\n",
    "    reviews_ints.append(ri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length review : 0\n",
      "Maximum length review: 159\n"
     ]
    }
   ],
   "source": [
    "# Just to check if there is any empty review\n",
    "# also check the max size to understand the size of the vector being used with RNN\n",
    "from collections import Counter\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length review : {}\".format(review_lens[0]))\n",
    "print(\"Maximum length review: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's truncate to  150 steps. For reviews shorter than 150, \n",
    "#we'll pad with 0s. For reviews longer than 150, we can truncate them to the first 150 characters.\n",
    "reviews_ints = [r[0:150] for r in reviews_ints if len(r) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Padding 0 for the reviews shorter than 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,    54, 14423,\n",
       "         4306, 10488,    67,   228,    31,   128,   237,   237,   775,\n",
       "        10488,   186,     9,  3278,  2374,  1246, 10488,   752,  3278,\n",
       "        45372,   296,   148,   967,  1339,  2051,   476,     5,   337,\n",
       "        10488,   166,    58,  1041,    59,   178,    86,    81,    31,\n",
       "          183,   126,   430,   457,   126],\n",
       "       [   99,   333,   128,    44,   367,  1374,   868,  6262,   382,\n",
       "         1556,   316,  1257,    62,   407,  1154,    64,  1788,  1792,\n",
       "          110,  1074,     2,  6035,    23,    42,   105,   615,   382,\n",
       "          186, 14011,  2693,    69,   748,   291,    64,   163,   138,\n",
       "          550,   109,   225,  1788,   875,     4,  1125,    43,   877,\n",
       "         1788,   114,   382,   283,    68],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,    69, 27856,   457,   275,   206,   590,   144,\n",
       "          688,    69,  1041,   118,   621,   536,    31,   709,    51,\n",
       "           34,   295,   124, 14183,   165],\n",
       "       [ 2747,     1,  4155,    83,   422,  1523,   170,  5849,     5,\n",
       "          300, 10734,     8,   685,    11,    14,   200,   412,     8,\n",
       "          685,   510,  1092,    37,    65,    38,     5,   201,   195,\n",
       "            4,   556,    56,     8,   685,   331,   277,    38,  6062,\n",
       "           10,   556,   336,    47,  1836,  2682, 45181,   486,     8,\n",
       "            1,   211,  2517,   230,   201],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,   190,\n",
       "        46768,   360,    79,  1029,  1856,   429,   335, 48162,  6920,\n",
       "          271,    16,   690,   641,   988,  1521,   174,   690, 18269,\n",
       "          177,  2672,  1535,  1296,    38,  2537,    23,    57,   101,\n",
       "           28,    22, 59652,   211,   277,  4973,  1369, 45487,   200,\n",
       "         1184,  6073,  2852,     9,    96]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len=150\n",
    "features = np.zeros((len(reviews_ints), seq_len), dtype =int)\n",
    "\n",
    "for i, row in enumerate(reviews_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "features[:5, 100:150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape (30000,)\n",
      "features shape (30000, 150)\n"
     ]
    }
   ],
   "source": [
    "labels = data_df['Rating']\n",
    "\n",
    "print (\"labels shape\", labels.shape)\n",
    "print (\"features shape\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(24000, 150) \n",
      "Test set: \t\t(6000, 150)\n",
      "label set: \t\t(24000,) \n",
      "Test label set: \t\t(6000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size =0.2, random_state=0)\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "\n",
    "print(\"label set: \\t\\t{}\".format(train_y.shape), \n",
    "      \"\\nTest label set: \\t\\t{}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining hyperparams\n",
    "\n",
    "lstm_size=128\n",
    "lstm_layers = 2\n",
    "batch_size =200\n",
    "learning_rate =0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66634"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = len(vocab_to_int)+1\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the graph object\n",
    "tf.reset_default_graph()\n",
    "with tf.name_scope('inputs'):\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name ='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name = 'labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name ='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_size =500\n",
    "\n",
    "with tf.name_scope(\"Embeddings\"):\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1,1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM Cell\n",
    "\n",
    "\n",
    "def lstm_cell():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size, reuse=tf.get_variable_scope().reuse)\n",
    "    # Add dropout to the cell\n",
    "    return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope(\"RNN_layers\"):\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(lstm_layers)])\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"RNN_Forward\"):\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Output\n",
    "\n",
    "We only care about the final output, we'll be using that as our sentiment prediction. So we need to grab the last output with outputs[:, -1], the calculate the cost from that and labels_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Prediction\"):\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:,-1], 1 , activation_fn=tf.sigmoid)\n",
    "    tf.summary.histogram(\"predictions\", predictions)\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost= tf.losses.mean_squared_error(labels_, predictions)\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"validation\"):\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy= tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "This is a simple function for returning batches from our data. First it removes data such that we only have full batches. Then it iterates through the x and y arrays and returns slices out of those arrays with size [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x,y,batch_size=2000):\n",
    "    n_batches=len(x)//batch_size\n",
    "    x,y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    \n",
    "    for ii in range (0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 50 Train loss: 0.189\n",
      "Epoch: 0/10 Iteration: 100 Train loss: 0.201\n",
      "Val acc: 0.403\n",
      "Epoch: 1/10 Iteration: 150 Train loss: 0.116\n",
      "Epoch: 1/10 Iteration: 200 Train loss: 0.096\n",
      "Epoch: 2/10 Iteration: 250 Train loss: 0.113\n",
      "Val acc: 0.305\n",
      "Epoch: 2/10 Iteration: 300 Train loss: 0.097\n",
      "Epoch: 2/10 Iteration: 350 Train loss: 0.090\n",
      "Val acc: 0.409\n",
      "Epoch: 3/10 Iteration: 400 Train loss: 0.053\n",
      "Epoch: 3/10 Iteration: 450 Train loss: 0.062\n",
      "Epoch: 4/10 Iteration: 500 Train loss: 0.038\n",
      "Val acc: 0.423\n",
      "Epoch: 4/10 Iteration: 550 Train loss: 0.043\n",
      "Epoch: 4/10 Iteration: 600 Train loss: 0.062\n",
      "Val acc: 0.462\n",
      "Epoch: 5/10 Iteration: 650 Train loss: 0.022\n",
      "Epoch: 5/10 Iteration: 700 Train loss: 0.054\n",
      "Epoch: 6/10 Iteration: 750 Train loss: 0.029\n",
      "Val acc: 0.496\n",
      "Epoch: 6/10 Iteration: 800 Train loss: 0.055\n",
      "Epoch: 7/10 Iteration: 850 Train loss: 0.052\n",
      "Val acc: 0.465\n",
      "Epoch: 7/10 Iteration: 900 Train loss: 0.038\n",
      "Epoch: 7/10 Iteration: 950 Train loss: 0.053\n",
      "Epoch: 8/10 Iteration: 1000 Train loss: 0.027\n",
      "Val acc: 0.511\n",
      "Epoch: 8/10 Iteration: 1050 Train loss: 0.048\n",
      "Epoch: 9/10 Iteration: 1100 Train loss: 0.029\n",
      "Val acc: 0.451\n",
      "Epoch: 9/10 Iteration: 1150 Train loss: 0.039\n",
      "Epoch: 9/10 Iteration: 1200 Train loss: 0.035\n"
     ]
    }
   ],
   "source": [
    "epochs =10\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter('./logs/tb/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/tb/test', sess.graph)\n",
    "    iteration =10\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x,y) in enumerate(get_batches(train_x, train_y, batch_size),1):\n",
    "            feed = {inputs_:x, labels_:y[:,None], keep_prob:0.3, initial_state:state}\n",
    "            summary, loss, state, _ = sess.run([merged,cost, final_state, optimizer],feed_dict=feed )\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            \n",
    "            if iteration%50==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "            \n",
    "            if iteration%125==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(test_x, test_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    summary, batch_acc, val_state = sess.run([merged, accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "            \n",
    "            test_writer.add_summary(summary, iteration)\n",
    "            saver.save(sess, \"checkpoints/reviews.ckpt\")\n",
    "    saver.save(sess,'checkpoints/reviews.ckpt')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/reviews.ckpt\n",
      " Test Accuracy: 0.430500\n"
     ]
    }
   ],
   "source": [
    "test_acc=[]\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"checkpoints/reviews.ckpt\")\n",
    "    test_state= sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    \n",
    "    for x, y in get_batches(test_x, test_y, batch_size):\n",
    "        feed = {inputs_: x,\n",
    "               labels_: y[:,None],\n",
    "               keep_prob:1, \n",
    "               initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    \n",
    "    print (\" Test Accuracy: {:3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try applying Bag of words vector in RNN\n",
    "## Similarly try apply vocab_to int vector in ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the bag of words\n",
      "(30000, 500)\n",
      "[[0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print (\"creating the bag of words\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Played with different max_features hyperparameter\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None\n",
    "                             ,max_features=500)\n",
    "train_data_features = vectorizer.fit_transform(clean_reviews_all_1)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "print ((train_data_features).shape)\n",
    "\n",
    "print (train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features[1][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abl', 'absolut', 'act', 'action', 'actual', 'add', 'age', 'ago', 'album', 'almost', 'along', 'alreadi', 'also', 'although', 'alway', 'amaz', 'amazon', 'american', 'anim', 'annoy', 'anoth', 'anyon', 'anyth', 'appear', 'around', 'arriv', 'ask', 'author', 'avail', 'away', 'babi', 'back', 'bad', 'bag', 'band', 'base', 'basic', 'batteri', 'beauti', 'becom', 'begin', 'believ', 'best', 'better', 'big', 'bit', 'black', 'book', 'bore', 'bought', 'box', 'boy', 'brand', 'break', 'buy', 'call', 'came', 'camera', 'cannot', 'car', 'care', 'case', 'cd', 'chang', 'chapter', 'charact', 'charg', 'cheap', 'check', 'child', 'children', 'christma', 'classic', 'clean', 'clear', 'close', 'collect', 'color', 'come', 'compani', 'compar', 'complet', 'comput', 'connect', 'consid', 'contain', 'continu', 'control', 'cool', 'copi', 'cost', 'could', 'coupl', 'cours', 'cover', 'creat', 'custom', 'cut', 'date', 'daughter', 'day', 'deal', 'decid', 'definit', 'describ', 'descript', 'design', 'detail', 'develop', 'die', 'differ', 'difficult', 'direct', 'disappoint', 'disc', 'dog', 'done', 'dont', 'drive', 'dvd', 'earli', 'easi', 'easili', 'edit', 'effect', 'either', 'els', 'end', 'enjoy', 'enough', 'entertain', 'entir', 'episod', 'especi', 'etc', 'even', 'ever', 'everi', 'everyon', 'everyth', 'exampl', 'excel', 'except', 'excit', 'expect', 'experi', 'extra', 'extrem', 'eye', 'face', 'fact', 'fall', 'famili', 'fan', 'far', 'fast', 'favorit', 'featur', 'feel', 'felt', 'figur', 'film', 'final', 'find', 'fine', 'finish', 'first', 'fit', 'follow', 'forward', 'found', 'four', 'free', 'friend', 'full', 'fun', 'funni', 'game', 'gave', 'gener', 'get', 'gift', 'girl', 'give', 'given', 'go', 'goe', 'good', 'got', 'great', 'guess', 'guy', 'half', 'hand', 'handl', 'happen', 'happi', 'hard', 'hate', 'head', 'hear', 'heard', 'heart', 'help', 'high', 'highli', 'histori', 'hit', 'hold', 'home', 'hope', 'hot', 'hour', 'hous', 'howev', 'huge', 'idea', 'import', 'impress', 'includ', 'inform', 'insid', 'instal', 'instead', 'interest', 'issu', 'item', 'job', 'keep', 'kid', 'kind', 'know', 'lack', 'larg', 'last', 'later', 'learn', 'least', 'leav', 'left', 'less', 'let', 'level', 'life', 'light', 'like', 'line', 'list', 'listen', 'littl', 'live', 'long', 'longer', 'look', 'lost', 'lot', 'love', 'low', 'lyric', 'machin', 'made', 'main', 'make', 'man', 'mani', 'materi', 'matter', 'may', 'mayb', 'mean', 'mention', 'might', 'mind', 'minut', 'miss', 'mix', 'model', 'money', 'month', 'move', 'movi', 'mr', 'much', 'music', 'must', 'name', 'need', 'never', 'new', 'next', 'nice', 'night', 'note', 'noth', 'notic', 'novel', 'number', 'offer', 'often', 'ok', 'old', 'one', 'open', 'opinion', 'order', 'origin', 'other', 'overal', 'packag', 'page', 'paper', 'part', 'past', 'pay', 'peopl', 'perfect', 'perform', 'person', 'phone', 'pick', 'pictur', 'piec', 'place', 'plastic', 'play', 'player', 'pleas', 'plot', 'plu', 'point', 'poor', 'pop', 'possibl', 'power', 'present', 'pretti', 'previou', 'price', 'print', 'probabl', 'problem', 'produc', 'product', 'program', 'provid', 'purchas', 'put', 'qualiti', 'question', 'quickli', 'quit', 'rate', 'rather', 'read', 'reader', 'real', 'realli', 'reason', 'receiv', 'recent', 'recommend', 'record', 'refer', 'releas', 'rememb', 'replac', 'rest', 'return', 'review', 'right', 'rock', 'room', 'run', 'said', 'save', 'saw', 'say', 'scene', 'school', 'screen', 'season', 'second', 'see', 'seem', 'seen', 'self', 'sens', 'seri', 'servic', 'set', 'sever', 'ship', 'short', 'show', 'side', 'simpl', 'simpli', 'sinc', 'sing', 'singl', 'size', 'slow', 'small', 'someon', 'someth', 'sometim', 'son', 'song', 'sound', 'special', 'spend', 'stand', 'star', 'start', 'state', 'stay', 'stick', 'still', 'stop', 'store', 'stori', 'stuff', 'style', 'subject', 'suggest', 'support', 'suppos', 'sure', 'surpris', 'system', 'take', 'talk', 'tast', 'tell', 'terribl', 'thank', 'thing', 'think', 'though', 'thought', 'three', 'time', 'titl', 'today', 'togeth', 'told', 'took', 'tool', 'top', 'total', 'toy', 'track', 'tri', 'true', 'truli', 'turn', 'tv', 'two', 'type', 'understand', 'unfortun', 'unit', 'unless', 'us', 'use', 'usual', 'version', 'video', 'view', 'voic', 'wait', 'want', 'war', 'wast', 'watch', 'water', 'way', 'week', 'well', 'went', 'white', 'whole', 'wish', 'within', 'without', 'women', 'wonder', 'word', 'work', 'world', 'worst', 'worth', 'would', 'write', 'written', 'wrong', 'ye', 'year', 'yet', 'young']\n"
     ]
    }
   ],
   "source": [
    "vocab= vectorizer.get_feature_names()\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features, data_df['Rating'], test_size =0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "RF_clf.fit(X_train,y_train)\n",
    "RFpred_train= RF_clf.predict(X_train)\n",
    "RFpred_test= RF_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.51002734609\n",
      "test score:  0.511411943919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print (\"train score: \", roc_auc_score(y_train,RFpred_train))\n",
    "print (\"test score: \", roc_auc_score(y_test,RFpred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_clf= LogisticRegression()\n",
    "LR_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.742686744053\n",
      "test score:  0.719628487843\n"
     ]
    }
   ],
   "source": [
    "LRpred_train= LR_clf.predict(X_train)\n",
    "LRpred_test= LR_clf.predict(X_test)\n",
    "\n",
    "print (\"train score: \", roc_auc_score(y_train,LRpred_train))\n",
    "print (\"test score: \", roc_auc_score(y_test,LRpred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
